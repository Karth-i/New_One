# -*- coding: utf-8 -*-
"""Without glove.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RPgQrJY-yReCKLhqD-tZURCYduWybcVS
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import spacy
import re
import string
import nltk
import torch
import seaborn as sns
import transformers
import json
import logging
from collections import defaultdict
import re
import numpy as np
import nltk
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import optimizers, layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Dropout, Flatten

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer

from torch import cuda
from tqdm import tqdm
from collections import Counter
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaModel, RobertaTokenizer
from sklearn.metrics import classification_report

from nltk.stem import PorterStemmer
nltk.download('punkt')
logging.basicConfig(level=logging.ERROR)

df = pd.read_excel("/content/LabeledText.xlsx")

"""**Preprocessing**"""

df_copy = df.copy()

def lowercase(data):
    return data['Caption'].str.lower()

def change_punctuation(data):
    return data['Caption'].str.replace('`', "'")

def remove_numbers(data):
    return data['Caption'].replace('[^a-zA-z.,!?/:;\"\'\s]', '', regex=True)

def remove_special_characters(data):
    return data['Caption'].replace('[^a-zA-Z0-9 ]', '', regex=True)

def custom(data):
    return data['Caption'].replace('im', 'i am')

nlp = spacy.load("en_core_web_sm")
def lemmatize(data):
    lemmatized_array = []

    for text in data['Caption']:
        lemmatized_text = []
        doc = nlp(text)
        for token in doc:
            lemmatized_text.append(token.lemma_)
        lemmatized_array.append(' '.join(lemmatized_text))
    return lemmatized_array

def stop_words(data):
    stop_words_array = []
    for text in data['Caption']:
        doc = nlp(text)
        filtered_tokens = [token.text for token in doc if not token.is_stop]
        stop_words_array.append(' '.join(filtered_tokens))
    return stop_words_array

def stem_words(data):
    stemmer = PorterStemmer()
    stemmed_array = []
    for text in data['Caption']:
        tokens = nltk.word_tokenize(text)
        stemmed_tokens = [stemmer.stem(token) for token in tokens]
        stemmed_array.append(' '.join(stemmed_tokens))
    return stemmed_array

def delete_links(data):
    return data['Caption'].replace(r'http\S+', '', regex=True)

def preprocessing(data):
    df['Caption'] = lowercase(df)
    df['Caption'] = custom(df)
    df['Caption'] = change_punctuation(df)
    df['Caption'] = lemmatize(df)
    df['Caption'] = remove_numbers(df)
    df['Caption'] = delete_links(df)
    df['Caption'] = stem_words(df)
    df['Caption'] = remove_special_characters(df)
    return df

df_copy = preprocessing(df_copy)

df_copy.drop_duplicates(subset=['Caption'], inplace=True)
df_copy['Caption'] = df_copy['Caption'].astype('str')

le = LabelEncoder()
df_copy['LABEL'] = le.fit_transform(df_copy['LABEL'])
df_copy

X = df_copy['Caption']
y = df_copy['LABEL']

print("\nTarget Labels:",y)

max_words = 9000
maxlen = 200
training_samples = int(len(X)*0.8)

text_dataset = tf.data.Dataset.from_tensor_slices(X)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=50)

max_features = 15000

vectorize_layer = tf.keras.layers.TextVectorization(
        max_tokens=max_words, # Max number of word in the internal dictionnary. We keep the most frequent
        output_mode='int',
        output_sequence_length=maxlen  # Size max of text
        )

vectorize_layer.adapt(text_dataset.batch(64))

voc = vectorize_layer.get_vocabulary()
word_index = dict(zip(voc, range(len(voc))))
c = 0
for i,j in word_index.items():
  print(i,j,end= " ")
  c += 1
  if c==21 or c==42:
    print("\n")

model1 = keras.Sequential([
    layers.Input(shape=(1,), dtype=tf.string),
    vectorize_layer,
    layers.Embedding(
        input_dim=max_words,
        output_dim=50,
        input_length=maxlen
    ),
    layers.Bidirectional(layers.GRU(128, return_sequences=True)),
    layers.GlobalMaxPooling1D(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dropout(0.4), #81, 0.5
    Dense(3, activation='softmax'),
])


model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

cl = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', restore_best_weights=True, patience=9)]

history = model1.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=150, batch_size=60, callbacks = cl)

history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss', 'val_loss']].plot()
print("\n")
history_frame.loc[:, ['accuracy', 'val_accuracy']].plot()

predictions = model1.predict(X_valid)
predicted_labels = np.argmax(predictions, axis=1)

accu = accuracy_score(predicted_labels, y_valid)
pre = precision_score(predicted_labels, y_valid,average = "weighted")
f1 = f1_score(predicted_labels, y_valid,average = "weighted")
print("Accuracy:",accu)
print("\nPrecision:",pre)
print("\nF1-Score:",f1)

# # Preprocess messages
# preprocessed_messages = []
# for user, messages in user_messages.items():
#     preprocessed_messages.extend(preprocess_messages(messages))

# # Ensure the shape of the preprocessed messages is correct
# preprocessed_messages = np.array(preprocessed_messages)

# # Reshape the preprocessed messages to have a 2D shape
# preprocessed_messages = preprocessed_messages.reshape((preprocessed_messages.shape[0], 1))

# # Predict sentiment for the messages
# predictions = model1.predict(preprocessed_messages)

# # Assign the predicted sentiment to each message
# sentiment_labels = [np.argmax(prediction) for prediction in predictions]

# # Print the predicted sentiments for each user
# idx = 0
# for user, messages in user_messages.items():
#     print(f"User: {user}")
#     for message in messages:
#         sentiment = le.inverse_transform([sentiment_labels[idx]])[0]
#         print(f"Message: {message}")
#         print(f"Predicted sentiment: {sentiment}")
#         idx += 1
#     print()

def process_whatsapp_file(file_path):
    user_messages = defaultdict(list)
    with open(file_path, 'r', encoding='utf-8') as file:
        # Read the lines of the WhatsApp chat history file
        lines = file.readlines()

    current_user = None
    for line in lines:
        # Split each line by the timestamp and sender's name
        parts = line.split(' - ')
        if len(parts) > 1:
            user_message = parts[1].strip()  # Extract the user name and message content
            user_parts = user_message.split(': ')
            if len(user_parts) > 1:
                user = user_parts[0].strip()  # Extract the user name
                message = ': '.join(user_parts[1:]).strip()  # Reconstruct the message part

                # Check if the user has changed
                if user != current_user:
                    # If the user has changed, reset the current_user variable
                    current_user = user
                    # Initialize an empty list for the user if it's the first message from that user
                    if not user_messages[current_user]:
                        user_messages[current_user] = []

                # Extract English words from the message and add them to the corresponding user's list
                english_words = extract_english_words(message)
                user_messages[current_user].extend(english_words)

    return user_messages

def extract_english_words(text):
    # Remove punctuation and numbers
    text = re.sub(r'[^\w\s]', '', text)
    # Tokenize the text
    words = text.split()
    # Filter out non-English words
    english_words = [word for word in words if word.isascii()]
    return english_words

def preprocess_messages(messages):
    # Lowercase the messages
    messages = [message.lower() for message in messages]
    # Tokenize the messages
    tokenized_messages = [nltk.word_tokenize(message) for message in messages]
    # Filter out non-English words
    english_messages = [[word for word in message if word.isascii()] for message in tokenized_messages]
    # Convert the messages to strings
    preprocessed_messages = [' '.join(message) for message in english_messages]
    return preprocessed_messages

# Path to your WhatsApp chat history file
whatsapp_file_path = "/content/WhatsApp Chat.txt"

# Process the WhatsApp chat history file
user_messages = process_whatsapp_file(whatsapp_file_path)

# Process the WhatsApp chat history file
user_messages = process_whatsapp_file("/content/WhatsApp Chat.txt")

# Preprocess messages
preprocessed_messages = []
for user, messages in user_messages.items():
    preprocessed_messages.extend(preprocess_messages(messages))

# Reshape the preprocessed messages to have a 2D shape
preprocessed_messages = np.array(preprocessed_messages).reshape((len(preprocessed_messages), 1))

# Print the user names
users = list(user_messages.keys())
for i, user in enumerate(users):
    print(f"{i+1}. {user}")

# Get user index input
user_index = int(input("Enter the index of the user to get sentiment for: ")) - 1

# Predict sentiment for the messages from the selected user
predictions = model1.predict(preprocessed_messages[user_index:user_index+1])

# Assign the predicted sentiment to each message
sentiment_labels = [np.argmax(prediction) for prediction in predictions]

# Print the predicted sentiment for the messages from the selected user
sentiment = le.inverse_transform([sentiment_labels[0]])[0]
print(f"Predicted sentiment for {users[user_index]}: {sentiment}")

new_np1 = model1

from google.colab import drive
drive.mount('/content/drive')

model1.save('/content/drive/MyDrive/new_np1.keras')

